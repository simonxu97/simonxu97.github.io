<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Resume - Start Bootstrap Theme</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Clarence Taylor</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile1.png" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Publication">Publication</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Research Project">Research Projects</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">Interests</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Ziqing Xu
                        <span class="text-primary">Simon</span>
                    </h1>
                    <div class="subheading mb-5">
                        5035 S East End Ave · Chicago, IL 60615 · (312) 860-5659 ·
                        <a href="mailto:name@email.com">simonxu@uchicago.edu</a>
                    </div>
                    <p class="lead mb-5">I'm a master student at University of Chicago, majoring in Statistics. I obtained Bachelor degree in Fudan University, majoring in Mathematics. My research interests are deep learning theory and optimization. I'm currently applying for Ph.D. in applied math and related fields.</p>
<!--                    <div class="social-icons">-->
<!--                        <a class="social-icon" href="#"><i class="fab fa-linkedin-in"></i></a>-->
<!--                        <a class="social-icon" href="#"><i class="fab fa-github"></i></a>-->
<!--                        <a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>-->
<!--                        <a class="social-icon" href="#"><i class="fab fa-facebook-f"></i></a>-->
<!--                    </div>-->
                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">University of Chicago</h3>
                            <div class="subheading mb-3">Master of Science</div>
                            <div>Statistics</div>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">September 2019 - June 2021(expect)</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Fudan University</h3>
                            <div class="subheading mb-3">Bachelor of Science</div>
                            <div>Mathematics</div>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">September 2015 - June 2019</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Publication-->
            <section class="resume-section" id="Publication">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publication</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Deep Transfer Tensor Decomposition with Orthogonal Constraint for Recommender</h3>
                            <div class="subheading mb-3">Collaborators: Zhenyu Chen, Professor Donglin Wang</div>
                            <div>Accepted by AAAI 2021</div>
                        </div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Research Project-->
            <section class="resume-section" id="Research Project">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Project</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Mean-field Analysis of Two-Layer Neural Networks for Partial Differential Equations</h3>
                            <div class="subheading mb-3">Advisor: Professor Yuehaw Khoo, Professor Haizhao Yang</div>
                            <p>Although neural networks achieve great success in scientific computing, there are very little theoretical analysis. This motivates me to fill in this blank. The difficulty of this project is that the differentiation puts weights outside activation functions so that the dynamics of probability distribution of weights might be unbounded. To conquer this problem, we consider gradient descent to minimize variational form in function space. By applying integral by part, we remove laplace operator on neural networks which solves the problems. We analyze the optimization process in fourier domain. Under the assumption that the true solution of Poisson equation is bandlimited, we show the L2 norm of low frequency part of neural network converges to true solution exponentially fast. The high frequency part of neural network diverges but if we apply early stopping, the norm of it will be bounded. After finishing the theory part, we also run some simulations to check whether our algorithm works in practice.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2020 - Present</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Stochastic VarPro for Neural Network</h3>
                            <div class="subheading mb-3">Advisor: Professor Lek-Heng Lim</div>
                            <p>Neural network can be viewed as nonlinear function composite with linear transform. Thus, the optimal weights in last layer is determined by all the other optimal weights according to basic linear regression theory. However, modern optimization methods, such as SGD and ADAM, don’t utilize this relation. We added randomness into Variable Projection (VarPro) method and tested whether it would be a better way to train neural network. To incorporate randomness, we sampled from whole dataset and updated a subset of Jacobian matrix in each iteration. We then compared our algorithm with other modern optimization methods like SGD, ADAM and Gauss-Newton on different tasks. The result is our algorithm sometimes outperformed other methods in certain tasks, like fitting high frequency functions. Now, we are still improving the algorithm and explore its best application.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">July 2020 - Present</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Accelerate Convergence of Neural Network Fitting High Frequency Functions</h3>
                            <div class="subheading mb-3">Advisor: Professor Yuehaw Khoo</div>
                            <p>According to Frequency Principle, neural network behaves poorly when fitting high frequency functions. We decided to optimize over initialization distribution so the high frequency spectrum of neural tangent kernel is maximized, which accelerated the convergence due to neural tangent kernel theory. Directly optimized over distribution is hard, so we discretized continuous distribution into dis- crete distribution over some points with equal size. Then, we empirically calculated corresponding NTK and used cvx to obtain optimal discrete distributions. To continuous the distribution, we first sampled weights according to discrete distribution and added Gaussian noise, of which variance is in- verse ratio to grip size. The simulations showed that our algorithm outperformed traditional gaussian intialization neural networks in the first ten thousand epochs but later stuck at local minimum. This phenomenon was due to the fact that our algorithm killed a nearly half of neurons at initialization. The early advantage could be seen as the our algorithm was doing optimization on a subspace of pa- rameters, so it might converge to the minimum faster but it would eventually be beaten by traditional method. A direct solution is that we can use other activation functions like sigmoid and tanh which doesn’t suffer from dead neuron issues. However, when I implement this change, the cvx couldn’t find optimal initialization. I think the issue is for ReLU network, the optimization problem is simply linear programming which can be solved efficiently. However, for sigmoid and tanh network, the time it takes to find optimal initialization can be very large, which explains why cvx might fail. Though this project turned out to be a dead end, I enjoyed process of designing and experimenting algorithms and applied it to tackle real-world problems. What’s more, this project leads me to think about can we propose a good activation function which we can find optimal initialization faster while it doesn’t suffer from dead neuron issues.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">June 2020 - August 2020</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Interests-->
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>I'm a big fan of Game of Go. I started to learn it when I was 6. I was quite astonished when AlphaGo defeated Lee Sedol in 2018. Nowadays, AlphaGo is the best teacher for every go player. I used it to analyze my game and learned a lot. I hope I can play with some professional go players one day. </p>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Elite Students Program - Fudan University
                        </li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Finalist Prize - 2016 The Mathematical Contest in modeling
                        </li>
                    </ul>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
